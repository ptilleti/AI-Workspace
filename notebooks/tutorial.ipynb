{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3cbac3",
   "metadata": {},
   "source": [
    "# üìö Building Your First AI Chatbot - Interactive Tutorial\n",
    "\n",
    "Welcome! This notebook will guide you through building an AI chatbot that can answer questions about PDF documents.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "1. How LLMs (Large Language Models) work\n",
    "2. Processing PDF documents\n",
    "3. Context Augmented Generation (CAG)\n",
    "4. Interacting with AI (locally with Ollama or via OpenAI API)\n",
    "5. Building a complete chatbot\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic Python knowledge\n",
    "- **Ollama installed** (free local models - recommended!) OR OpenAI API key\n",
    "- A PDF file to test with\n",
    "\n",
    "## üÜì This Tutorial Uses Ollama (Free!)\n",
    "We'll use Ollama to run AI models locally on your computer - no API costs, no usage limits!\n",
    "Already installed Ollama and pulled llama3.2? Great! Otherwise, see `../OLLAMA_SETUP.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a57df",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Installation\n",
    "\n",
    "First, let's make sure we have all the required packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this once)\n",
    "# Uncomment the line below if packages are not installed\n",
    "# !pip install openai python-dotenv pypdf\n",
    "\n",
    "# Note: The 'openai' package is used for both OpenAI API and Ollama\n",
    "# (Ollama has an OpenAI-compatible API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d2906",
   "metadata": {},
   "source": [
    "### Quick Setup Check\n",
    "\n",
    "Make sure your `.env` file is configured. For Ollama (free):\n",
    "```\n",
    "LLM_PROVIDER=ollama\n",
    "OLLAMA_MODEL=llama3.2\n",
    "```\n",
    "\n",
    "For OpenAI (requires API key):\n",
    "```\n",
    "LLM_PROVIDER=openai\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd16ef",
   "metadata": {},
   "source": [
    "## Step 2: Understanding PDFs and Text Extraction\n",
    "\n",
    "Before we can chat about a PDF, we need to extract its text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "\n",
    "# Let's extract text from a PDF\n",
    "pdf_path = \"../data/sample.pdf\"  # Change this to your PDF\n",
    "\n",
    "# Check if file exists\n",
    "if Path(pdf_path).exists():\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = pypdf.PdfReader(file)\n",
    "        \n",
    "        print(f\"üìÑ PDF Info:\")\n",
    "        print(f\"   Pages: {len(pdf_reader.pages)}\")\n",
    "        print(f\"   Metadata: {pdf_reader.metadata}\")\n",
    "        \n",
    "        # Extract first page text\n",
    "        first_page = pdf_reader.pages[0]\n",
    "        text = first_page.extract_text()\n",
    "        \n",
    "        print(f\"\\nüìù First 500 characters:\")\n",
    "        print(text[:500])\n",
    "else:\n",
    "    print(\"‚ùå PDF file not found!\")\n",
    "    print(\"üí° Add a PDF to the data/ folder and update the path above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8257a8",
   "metadata": {},
   "source": [
    "## Step 3: Understanding LLMs (Large Language Models)\n",
    "\n",
    "### What is an LLM?\n",
    "- A neural network trained on massive amounts of text\n",
    "- Predicts the next word/token based on previous context\n",
    "- Examples: GPT-4, Claude, Llama, Gemini\n",
    "\n",
    "### Key Concepts:\n",
    "- **Tokens**: Words or word pieces (1 token ‚âà 4 characters)\n",
    "- **Context Window**: Max tokens the model can process at once\n",
    "- **Temperature**: Controls randomness (0=deterministic, 1=creative)\n",
    "- **System Prompt**: Instructions for how the AI should behave\n",
    "- **User Prompt**: Your actual question or request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Choose your provider: 'ollama' (free, local) or 'openai' (paid, cloud)\n",
    "provider = os.getenv(\"LLM_PROVIDER\", \"ollama\")\n",
    "\n",
    "if provider == \"ollama\":\n",
    "    # Initialize Ollama client (no API key needed!)\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"  # Dummy key (Ollama doesn't need real keys)\n",
    "    )\n",
    "    model = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "    print(f\"ü¶ô Using Ollama with {model}\")\n",
    "    print(\"üí° Make sure Ollama is running: ollama serve\")\n",
    "else:\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    model = \"gpt-4o-mini\"\n",
    "    print(f\"ü§ñ Using OpenAI with {model}\")\n",
    "\n",
    "# Simple example: asking the LLM a question\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what an AI agent is in one simple sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nü§ñ AI Response:\\n{answer}\")\n",
    "\n",
    "# Show token usage if available (OpenAI provides this, Ollama may not)\n",
    "if hasattr(response, 'usage') and response.usage:\n",
    "    print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204abdcc",
   "metadata": {},
   "source": [
    "## Step 4: Context Augmented Generation (CAG)\n",
    "\n",
    "### The Magic of CAG:\n",
    "1. Extract text from PDF\n",
    "2. Send the text + question to the LLM\n",
    "3. LLM answers based on the provided context\n",
    "\n",
    "### CAG vs RAG:\n",
    "- **CAG**: Send entire document as context (simple, works for smaller docs)\n",
    "- **RAG**: Search for relevant chunks, only send those (better for large docs)\n",
    "\n",
    "For learning, CAG is perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract full PDF text\n",
    "def extract_full_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = pypdf.PdfReader(file)\n",
    "        text_parts = []\n",
    "        \n",
    "        for page in pdf_reader.pages:\n",
    "            text_parts.append(page.extract_text())\n",
    "        \n",
    "        return \"\\n\\n\".join(text_parts)\n",
    "\n",
    "# Try it (if you have a PDF)\n",
    "if Path(pdf_path).exists():\n",
    "    full_text = extract_full_pdf(pdf_path)\n",
    "    \n",
    "    print(f\"üìä Extraction Stats:\")\n",
    "    print(f\"   Characters: {len(full_text):,}\")\n",
    "    print(f\"   Words: {len(full_text.split()):,}\")\n",
    "    print(f\"   Est. Tokens: {len(full_text) // 4:,}\")\n",
    "    \n",
    "    # Store for next step\n",
    "    pdf_content = full_text\n",
    "else:\n",
    "    print(\"Add a PDF to test this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e04f0",
   "metadata": {},
   "source": [
    "## Step 5: Building the Chatbot!\n",
    "\n",
    "Now let's put it all together and create our PDF chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0426daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_pdf_question(question, pdf_content):\n",
    "    \"\"\"\n",
    "    Ask a question about the PDF content.\n",
    "    This is CAG in action!\n",
    "    \"\"\"\n",
    "    # Get provider settings\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"ollama\")\n",
    "    \n",
    "    if provider == \"ollama\":\n",
    "        client = OpenAI(\n",
    "            base_url=\"http://localhost:11434/v1\",\n",
    "            api_key=\"ollama\"\n",
    "        )\n",
    "        model = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "    else:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        model = \"gpt-4o-mini\"\n",
    "    \n",
    "    # Build the messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant that answers questions based on \"\n",
    "                \"the provided document. Use only information from the document. \"\n",
    "                \"If the answer isn't in the document, say so clearly.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Document content:\\n\\n{pdf_content}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Get response\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Get tokens if available\n",
    "    tokens = None\n",
    "    if hasattr(response, 'usage') and response.usage:\n",
    "        tokens = response.usage.total_tokens\n",
    "    \n",
    "    return answer, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chatbot!\n",
    "if 'pdf_content' in locals():\n",
    "    question = \"What is this document about? Give me a brief summary.\"\n",
    "    \n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    answer, tokens = ask_pdf_question(question, pdf_content)\n",
    "    print(f\"ü§ñ Answer: {answer}\")\n",
    "    \n",
    "    if tokens:\n",
    "        print(f\"\\nüìä Tokens used: {tokens}\")\n",
    "else:\n",
    "    print(\"Add a PDF first to test the chatbot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba545357",
   "metadata": {},
   "source": [
    "## Step 6: Try Your Own Questions!\n",
    "\n",
    "Now you can ask any question about your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive question answering\n",
    "if 'pdf_content' in locals():\n",
    "    # Try different questions:\n",
    "    questions = [\n",
    "        \"What are the main topics covered?\",\n",
    "        \"List any key concepts or terms\",\n",
    "        \"What's the most important takeaway?\"\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚ùì {q}\")\n",
    "        print('='*60)\n",
    "        answer, _ = ask_pdf_question(q, pdf_content)\n",
    "        print(f\"ü§ñ {answer}\")\n",
    "else:\n",
    "    print(\"Add a PDF to the data/ folder to start chatting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcb442",
   "metadata": {},
   "source": [
    "## üéì What You've Learned!\n",
    "\n",
    "Congratulations! You've now built a working AI chatbot. Here's what you learned:\n",
    "\n",
    "1. **PDF Processing**: How to extract text from documents\n",
    "2. **LLM Interaction**: How to send prompts and get responses (locally or via API)\n",
    "3. **CAG**: How to use context to make AI more accurate\n",
    "4. **Prompt Engineering**: Crafting system and user prompts\n",
    "5. **Free AI**: Using Ollama to run models locally without costs\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Ready to level up? Here are ideas:\n",
    "\n",
    "1. **Try Different Models**: Use `ollama pull qwen2.5:7b` for better quality\n",
    "2. **Add RAG**: Learn vector databases (ChromaDB) for large documents\n",
    "3. **Memory**: Make the chatbot remember conversation history\n",
    "4. **Multi-PDF**: Handle multiple documents at once\n",
    "5. **Web Interface**: Build a UI with Streamlit or Gradio\n",
    "6. **Advanced Agents**: Add tools, function calling, chains\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Ollama](https://ollama.ai/) - Free local models (what we used!)\n",
    "- [Ollama Model Library](https://ollama.ai/library) - Browse available models\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs) - If using OpenAI\n",
    "- [LangChain](https://python.langchain.com/) - Framework for LLM apps\n",
    "- [ChromaDB](https://www.trychroma.com/) - Vector database for RAG\n",
    "\n",
    "## üí° Model Recommendations\n",
    "\n",
    "### For PDF Q&A:\n",
    "- **llama3.2** (2GB) - Great balance, 128K context window ‚≠ê\n",
    "- **qwen2.5:7b** (4.7GB) - Better for technical docs\n",
    "- **phi3:medium** (7.9GB) - Best quality (needs 16+ GB RAM)\n",
    "\n",
    "Try different models:\n",
    "```bash\n",
    "ollama pull qwen2.5:7b\n",
    "```\n",
    "\n",
    "Then change `.env`: `OLLAMA_MODEL=qwen2.5:7b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43526b",
   "metadata": {},
   "source": [
    "## üí° Exercises\n",
    "\n",
    "Try these challenges:\n",
    "\n",
    "1. **Model Comparison**: Try llama3.2, qwen2.5:7b, and phi3 on the same question - compare speed and quality\n",
    "2. **Temperature Experiments**: Modify the temperature parameter (0.0-1.0) - how does it change responses?\n",
    "3. **System Prompts**: Try different system prompts - make it formal, casual, expert-level, or funny\n",
    "4. **Context Window**: Count tokens in your PDF - will it fit in the 128K context window?\n",
    "5. **Error Handling**: Add handling for when questions aren't answerable from the PDF\n",
    "6. **Conversation History**: Store previous Q&A pairs and include them in context\n",
    "7. **Multi-Model**: Build a function that asks multiple models and compares their answers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
